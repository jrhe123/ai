{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f \\left( x - \\epsilon f'(x) \\right) < f(x) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单层感应器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数 \n",
    "𝐿\n",
    "L：\n",
    "\n",
    "$$ L = \\frac{1}{2} (Y - y)^2 $$\n",
    "预测值 \n",
    "𝑦\n",
    "y 的定义：\n",
    "\n",
    "$$ y = f(wx + b) = \\text{sign}(wx + b) $$\n",
    "基于梯度下降法的参数更新公式：\n",
    "\n",
    "$$ w_{t+1} = w_t + \\Delta w = w_t - \\tau \\frac{\\partial L}{\\partial w} = w_t - \\tau \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial w} $$\n",
    "更新公式的近似形式：\n",
    "\n",
    "$$ w_{t+1} \\approx w_t + \\tau (Y - y) x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 某一层的x与y是非线性关系\n",
    "- 某一层的x与上一层的y是线性关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ x_j = \\sum_k w_{ji}^l y_i^{l-1} + \\theta_j^l $$\n",
    "\n",
    "\n",
    "$$ y_j = f(x_j) $$\n",
    "其中 𝑓 表示激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变量说明\n",
    "- \\( w_{ji}^l \\)：第 \\( l-1 \\) 层第 \\( i \\) 个神经元到第 \\( l \\) 层第 \\( j \\) 个神经元的权重。\n",
    "- \\( x_j \\)：第 \\( l \\) 层第 \\( j \\) 个神经元的输入。\n",
    "- \\( y_j \\)：第 \\( l \\) 层第 \\( j \\) 个神经元的输出。\n",
    "- \\( \\theta_j^l \\)：第 \\( l \\) 层第 \\( j \\) 个神经元的偏置。\n",
    "- \\( C \\)：损失函数（代价函数）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "当前层的误差项 \n",
    "$$ \\delta_j^l = \\frac{\\partial C}{\\partial x_j^l} = \\sum_i \\frac{\\partial C}{\\partial x_i^{l+1}} \\frac{\\partial x_i^{l+1}}{\\partial x_j^l} = \\sum_i \\delta_i^{l+1} \\frac{\\partial x_i^{l+1}}{\\partial x_j^l} $$\n",
    "\n",
    "上一层神经元输入 \n",
    "$$ x_i^{l+1} = \\sum_j w_{ji}^l y_j^l + \\theta_i^{l+1} $$\n",
    "\n",
    "导数项\n",
    "$$ \\frac{\\partial x_i^{l+1}}{\\partial x_j^l} = w_{ji}^l f'(x_j^l) $$\n",
    "\n",
    "变量说明\n",
    "- \\( \\delta_j^l \\)：第 \\( l \\) 层第 \\( j \\) 个神经元的误差项。\n",
    "- \\( C \\)：损失函数。\n",
    "- \\( x_j^l \\)：第 \\( l \\) 层第 \\( j \\) 个神经元的输入。\n",
    "- \\( x_i^{l+1} \\)：第 \\( l+1 \\) 层第 \\( i \\) 个神经元的输入。\n",
    "- \\( y_j^l \\)：第 \\( l \\) 层第 \\( j \\) 个神经元的输出。\n",
    "- \\( w_{ji}^l \\)：第 \\( l \\) 层第 \\( j \\) 个神经元到第 \\( l+1 \\) 层第 \\( i \\) 个神经元的权重。\n",
    "- \\( \\theta_i^{l+1} \\)：第 \\( l+1 \\) 层第 \\( i \\) 个神经元的偏置。\n",
    "- \\( f'(x_j^l) \\)：激活函数 \\( f \\) 对输入 \\( x_j^l \\) 的导数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 误差项递推公式：\n",
    "$$ \\delta_j^l = \\sum_i w_{ji}^{l+1} \\delta_i^{l+1} f'(x_j^l) $$\n",
    "\n",
    "- 当前层的误差项由上一层神经元的误差项加权和计算得到，同时乘以激活函数的梯度。\n",
    "- f'：激活函数的倒数\n",
    "- 误差值：$$ \\delta_j^l $$\n",
    "\n",
    "\n",
    "### 损失函数对权重的偏导数\n",
    "$$ \\frac{\\partial C}{\\partial w_{ji}} = \\delta_j^l y_i^{l-1} $$\n",
    "\n",
    "- 算出某一节点的梯度\n",
    "\n",
    "### 权重更新公式 (我们最终需要的)\n",
    "$$ w_{ji} = w_{ji} - \\eta \\frac{\\partial C}{\\partial w_{ji}} = w_{ji} - \\eta \\delta_j^l y_i^{l-1} $$\n",
    "\n",
    "其中 𝜂 是学习率，表示更新步长。\n",
    "\n",
    "\n",
    "\n",
    "- \\( \\delta_j^l \\)：第 \\( l \\) 层第 \\( j \\) 个神经元的误差项。\n",
    "- \\( w_{ji}^{l+1} \\)：第 \\( l+1 \\) 层第 \\( j \\) 个神经元到第 \\( l+1 \\) 层第 \\( i \\) 个神经元的权重。\n",
    "- \\( f'(x_j^l) \\)：激活函数 \\( f \\) 对第 \\( l \\) 层第 \\( j \\) 个神经元输入 \\( x_j^l \\) 的梯度。\n",
    "- \\( \\eta \\)：学习率，用于控制每次更新的步长。\n",
    "- \\( y_i^{l-1} \\)：第 \\( l-1 \\) 层第 \\( i \\) 个神经元的输出。\n",
    "- \\( C \\)：损失函数，表示模型的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 序列神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐藏状态的更新公式：\n",
    "$$ h_t = \\tanh(W_xh x_t + W_hh h_{t-1}) $$\n",
    "\n",
    "输出的计算公式：\n",
    "$$ y_t = \\tanh(W_o h_t) $$\n",
    "\n",
    "- \\( x_t \\)：长度为 \\( I \\) 的输入向量。\n",
    "- \\( h_t \\)：长度为 \\( H \\) 的隐藏状态向量。\n",
    "- \\( W_xh \\)：输入到隐藏层的权重矩阵，维度为 \\( H \\times I \\)。\n",
    "- \\( W_hh \\)：隐藏层到隐藏层的权重矩阵，维度为 \\( H \\times H \\)。\n",
    "- \\( W_o \\)：隐藏状态到输出的权重矩阵，维度为 \\( K \\times H \\)，其中 \\( K \\) 是输出维度。\n",
    "- \\( y_t \\)：当前时间步的输出向量。\n",
    "- \\( \\tanh \\)：双曲正切激活函数，用于引入非线性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- h(t-1) 隐藏层输入\n",
    "- x(t) 输入层输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM 长短期记忆网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遗忘门 (Forget Gate)：\n",
    "$$ f_t = \\sigma(W_x^f x_t + W_h^f h_{t-1}) $$\n",
    "\n",
    "- 0: 忘记所有信息\n",
    "- 1: 不遗忘任何东西\n",
    "\n",
    "遗忘门决定了上一个状态中哪些信息需要遗忘：\n",
    "$$ f_t \\cdot c_{t-1} $$\n",
    "\n",
    "输入门 (Input Gate)：\n",
    "$$ i_t = \\sigma(W_x^i x_t + W_h^i h_{t-1}) $$\n",
    "\n",
    "新信息的候选值：\n",
    "$$ \\tilde{c}_t = \\tanh(W_x^c x_t + W_h^c h_{t-1}) $$\n",
    "\n",
    "更新信息：\n",
    "$$ c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tilde{c}_t $$\n",
    "\n",
    "\n",
    "输出门 (Output Gate)：\n",
    "$$ o_t = \\sigma(W_x^o x_t + W_h^o h_{t-1}) $$\n",
    "\n",
    "输出状态 (Hidden State)：\n",
    "$$ h_t = o_t \\cdot \\tanh(c_t) $$\n",
    "\n",
    "- \\( f_t \\)：遗忘门的输出，决定遗忘多少上一时间步的信息。\n",
    "- \\( i_t \\)：输入门的输出，决定当前输入多少新信息。\n",
    "- \\( \\tilde{c}_t \\)：候选记忆状态，表示潜在的新信息。\n",
    "- \\( c_t \\)：当前的记忆单元状态，综合了上一状态和新信息。\n",
    "- \\( o_t \\)：输出门的输出，决定当前时间步输出多少信息。\n",
    "- \\( h_t \\)：隐藏状态，LSTM 的最终输出。\n",
    "- \\( \\sigma \\)：Sigmoid 激活函数，用于归一化值到 [0,1] 范围。\n",
    "- \\( \\tanh \\)：双曲正切激活函数，用于引入非线性。\n",
    "- \\( W_x^f, W_h^f \\)：输入到遗忘门、隐藏状态到遗忘门的权重矩阵。\n",
    "- \\( W_x^i, W_h^i \\)：输入到输入门、隐藏状态到输入门的权重矩阵。\n",
    "- \\( W_x^c, W_h^c \\)：输入到候选记忆状态、隐藏状态到候选记忆状态的权重矩阵。\n",
    "- \\( W_x^o, W_h^o \\)：输入到输出门、隐藏状态到输出门的权重矩阵。\n",
    "- \\( x_t \\)：当前时间步的输入。\n",
    "- \\( h_{t-1} \\)：前一时间步的隐藏状态。\n",
    "- \\( c_{t-1} \\)：前一时间步的记忆单元状态。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU 门控循环单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新门 (Update Gate)：\n",
    "$$ z_t = \\sigma(W_x^z x_t + W_h^z h_{t-1}) $$\n",
    "\n",
    "重置门 (Reset Gate)：\n",
    "$$ r_t = \\sigma(W_x^r x_t + W_h^r h_{t-1}) $$\n",
    "\n",
    "新信息的候选状态：\n",
    "$$ \\tilde{h}_t = \\tanh(W_x^c x_t + r_t \\cdot (W_h^c h_{t-1})) $$\n",
    "\n",
    "当前时间步的隐藏状态 (Output)：\n",
    "$$ h_t = z_t \\cdot h_{t-1} + (1 - z_t) \\cdot \\tilde{h}_t $$\n",
    "\n",
    "- \\( z_t \\)：更新门的输出，控制上一隐藏状态 \\( h_{t-1} \\) 和候选状态 \\( \\tilde{h}_t \\) 的结合比例。\n",
    "- \\( r_t \\)：重置门的输出，控制上一隐藏状态 \\( h_{t-1} \\) 在生成候选状态 \\( \\tilde{h}_t \\) 时的重要性。\n",
    "- \\( \\tilde{h}_t \\)：候选隐藏状态，表示当前时间步的潜在新信息。\n",
    "- \\( h_t \\)：当前时间步的隐藏状态输出，结合了上一时间步的信息和当前输入。\n",
    "- \\( h_{t-1} \\)：前一时间步的隐藏状态。\n",
    "- \\( x_t \\)：当前时间步的输入。\n",
    "- \\( W_x^z, W_h^z \\)：输入到更新门和隐藏状态到更新门的权重矩阵。\n",
    "- \\( W_x^r, W_h^r \\)：输入到重置门和隐藏状态到重置门的权重矩阵。\n",
    "- \\( W_x^c, W_h^c \\)：输入到候选状态和隐藏状态到候选状态的权重矩阵。\n",
    "- \\( \\sigma \\)：Sigmoid 激活函数，用于归一化值到 [0,1] 范围。\n",
    "- \\( \\tanh \\)：双曲正切激活函数，用于引入非线性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 第 l 层的输入与输出：\n",
    "\n",
    "第 \\( l \\) 层第 \\( p \\) 通道的输入 \\( z^{(l,p)} \\) 是通过对上一层所有通道的输出 \\( x^{(l-1,d)} \\) 与当前层的卷积核 \\( W^{(l,p,d)} \\) 进行卷积操作 (\\( \\otimes \\))，再加上偏置项 \\( b^{(l,p)} \\) 计算得到。\n",
    "\n",
    "\n",
    "$$ z^{(l,p)} = \\sum_{d=1}^D W^{(l,p,d)} \\otimes x^{(l-1,d)} + b^{(l,p)} $$\n",
    "\n",
    "\n",
    "$$ x^{(l,d)} = f\\left(z^{(l,d)}\\right) $$\n",
    "\n",
    "第 \\( l \\) 层第 \\( d \\) 通道的输出 \\( x^{(l,d)} \\) 是通过将输入 \\( z^{(l,d)} \\) 经过激活函数 \\( f \\) 计算得到。\n",
    "\n",
    "\n",
    "2. 损失函数对卷积核的梯度：\n",
    "$$ \\frac{\\partial L}{\\partial W^{(l,p,d)}} = \\delta^{(l,p)} \\otimes x^{(l-1,d)} $$\n",
    "\n",
    "卷积核的梯度 \\( \\frac{\\partial L}{\\partial W^{(l,p,d)}} \\) 是通过当前层误差项 \\( \\delta^{(l,p)} \\) 与上一层输出 \\( x^{(l-1,d)} \\) 的卷积计算得到。\n",
    "\n",
    "3.当前层的误差项递推公式：\n",
    "\n",
    "$$ \\delta^{(l,d)} = f'\\left(z^{(l,d)}\\right) \\cdot \\sum_{p=1}^P \\text{rot180}\\left(W^{(l+1,p,d)}\\right) \\otimes \\delta^{(l+1,p)} $$\n",
    "\n",
    "当前层第 \\( d \\) 通道的误差项 \\( \\delta^{(l,d)} \\) 是通过激活函数的导数 \\( f'(z^{(l,d)}) \\) 与下一层所有通道的误差项 \\( \\delta^{(l+1,p)} \\) 以及对应卷积核旋转 180 度后的卷积计算得到。\n",
    "\n",
    "- \\( z^{(l,p)} \\)：第 \\( l \\) 层第 \\( p \\) 通道的输入。\n",
    "- \\( x^{(l,d)} \\)：第 \\( l \\) 层第 \\( d \\) 通道的输出。\n",
    "- \\( W^{(l,p,d)} \\)：第 \\( l \\) 层第 \\( p \\) 通道与第 \\( d \\) 通道的卷积核。\n",
    "- \\( b^{(l,p)} \\)：第 \\( l \\) 层第 \\( p \\) 通道的偏置。\n",
    "- \\( \\delta^{(l,p)} \\)：第 \\( l \\) 层第 \\( p \\) 通道的误差。\n",
    "- \\( f \\)：激活函数，例如 ReLU 或 Sigmoid。\n",
    "- \\( f'(z) \\)：激活函数的导数。\n",
    "- \\( \\otimes \\)：卷积操作。\n",
    "- \\( \\text{rot180} \\)：卷积核旋转 180 度操作。\n",
    "- \\( \\frac{\\partial L}{\\partial W^{(l,p,d)}} \\)：损失函数对卷积核的梯度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling 池化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "池化层的误差反向传播：\n",
    "\n",
    "$$ \\delta^{(l,p)} = f'\\left(z^{(l,p)}\\right) \\odot \\text{up}\\left(\\delta^{(l+1,p)}\\right) $$\n",
    "\n",
    "上采样操作 (up) 的作用：\n",
    "上采样操作 \\( \\text{up} \\) 用于将下一层误差 \\( \\delta^{(l+1,p)} \\) 映射回池化操作之前的输入尺寸，以确保梯度正确传播。\n",
    "\n",
    "\n",
    "- \\( \\delta^{(l,p)} \\)：第 \\( l \\) 层第 \\( p \\) 通道的误差项。\n",
    "- \\( z^{(l,p)} \\)：第 \\( l \\) 层第 \\( p \\) 通道的输入。\n",
    "- \\( \\delta^{(l+1,p)} \\)：第 \\( l+1 \\) 层第 \\( p \\) 通道的误差项。\n",
    "- \\( f \\)：激活函数，例如 ReLU 或 Sigmoid。\n",
    "- \\( f'(z) \\)：激活函数的导数。\n",
    "- \\( \\odot \\)：元素逐项相乘操作。\n",
    "- \\( \\text{up} \\)：上采样函数，用于逆向映射池化区域。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
