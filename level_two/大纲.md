1. 数据初始化 (data initialization)

- 全零初始化
- 随机初始化
- 标准初始化: 标准方差
- Xavier: 适合 sigmod / tanh
- MSRA: 适合 Relu

2. 激活函数 (activation function)

- sigmod / tanh
- relu (死亡状态)
- leaky relu
- Parametric relu
- Random relu
- Exponential linear unit (ELU): 计算量太大
- maxout
- swish: x \* sigmoid (Bx): 自动学习的 beta

3. 标准化 (normalization)

- 图片对比度 / 保证数据在 0-1 之间， 稳定分布 / 去除量纲干扰
- Batch normalization (BN): batch size (同样)
  - Batch renormalization
- Layer normalization (LN): 每个样本 (nlp 长输入)
- Group normalization (GN): 每个样本的每个通道的每个组 （小 batch）
- Instance normalization (IN): 每个样本的每个通道 （图片生成，风格转变）

4. 正则化 (regularization)

- 泛化 (generalization): prevent overfitting
- dropout: 随机失活
- dropconnect: 随机失活权重
- 显示方法：提前终止 (early stopping)
- ensemble: 模型融合
  - k-fold
- maxout: 取 n 个激活的最大值
- 显示方法：参数正则化: l1, l2
- 隐式正则化：数据标准化, 图片增强, 随机参数更新, 标签平滑

5. 学习率 (learning rate)

- 步长
- 学习率衰减: 0.1, 0.01, 0.001

6. 最优化方法 (optimization)
   > > > 更好的更新方向

- SGD 随机梯度下降法
- momentum 动量法
- Nesterov accerlated gradient 法 (NAG)

  > > > 得到合理的学习率

- Adagrad 法
- Adadelta 法 & RMSprop 法
- Adam, Adamax, Nadam 法
